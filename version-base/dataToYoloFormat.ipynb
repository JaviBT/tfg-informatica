{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Data from RDD2022 to YOLO Format\n",
    "<h4>\n",
    "En este notebook preparamos los datos que vamos a utilizar para entrenar nuestros modelos YOLO. Los entrenamientos se van a realizar en Google Colabs y posteriormente nos descargaremos los pesos para utilizarlos localmente en la detección de objetos en nuestras imágenes test. Es por ello que vamos a ignorar las imágenes test en el datasets que subimos a Google Drive.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../datasets/RDD2022/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: ../datasets/RDD2022/images/Czech/train: Permission denied\n",
      "rm: ../datasets/RDD2022/images/Czech: Permission denied\n",
      "rm: ../datasets/RDD2022/images: Permission denied\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove contents inside ../datasets/RDD2022/\n",
    "os.system('rm -r ../datasets/RDD2022/*')\n",
    "\n",
    "# Copy ../data/RDD2022/ folder to ../datasets/RDD2022/\n",
    "os.system('cp -r ../data/RDD2022/ ../datasets/RDD2022/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing problematic files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:13<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating anotations in YOLO format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:11<00:00,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. Problematic files: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "subdatasets = ['China_Drone', 'China_MotorBike', 'Czech', 'India', 'Japan', 'United_States']\n",
    "\n",
    "class_name_to_id_dict = {\n",
    "    'D00': 0,\n",
    "    'D10': 1,\n",
    "    'D20': 2,\n",
    "    'D40': 3,\n",
    "}\n",
    "class_list = list(class_name_to_id_dict.keys())\n",
    "\n",
    "problematic_files = []\n",
    "\n",
    "print('Removing problematic files...')\n",
    "# Remove any object in anotations that has a name not in class list\n",
    "class_list = class_name_to_id_dict.keys()\n",
    "for subdataset in tqdm(subdatasets):\n",
    "    train_path = os.path.join(path, subdataset, 'train')\n",
    "    test_path = os.path.join(path, subdataset, 'test')\n",
    "    anotations_path = os.path.join(train_path, 'annotations')\n",
    "    labels_path = os.path.join(train_path, 'labels')\n",
    "    xmls_path = os.path.join(anotations_path, 'xmls')\n",
    "    file_names = os.listdir(xmls_path)\n",
    "\n",
    "    # Check if the {subdataset}/test folder exist and delete it\n",
    "    if os.path.exists(test_path):\n",
    "        os.system(f'rm -r {test_path}')\n",
    "\n",
    "    # For each xml file, check every object and remove those that have names not in class list\n",
    "    for file_name in file_names:\n",
    "        try:\n",
    "            with open(os.path.join(xmls_path, file_name), 'r') as xml:\n",
    "                tree = ET.parse(xml)\n",
    "                root = tree.getroot()\n",
    "                for obj in root.findall('object'):\n",
    "                    class_name = obj.find('name').text\n",
    "                    if class_name not in class_list:\n",
    "                        root.remove(obj)\n",
    "                    # He decidido dejar aquellas imagenes de background que no tienen ningun objeto para que el modelo aprenda a distinguirlo. No obstante,\n",
    "                    # es posible que sea necesario eliminar algunas ya que al solo quedarnos con cuatro etiquetas, es posible que haya muchas imagenes de background.\n",
    "                    tree.write(os.path.join(xmls_path, file_name)) # Save the xml with the removed objects\n",
    "        except Exception as e:\n",
    "            problematic_files.append(file_name)\n",
    "            continue\n",
    "\n",
    "# For each subdataset open the train/anotations/xmls folder and add to train/labels/ a file_name.txt with the labels in YOLO format\n",
    "# For example, for China_Drone_000000.xml we will create a file China_Drone_000000.txt with the anotations in YOLO format at train/labels/\n",
    "# YOLO format: class x_center y_center width height with values normalized between 0 and 1\n",
    "print('Creating anotations in YOLO format...')\n",
    "for subdataset in tqdm(subdatasets):\n",
    "    train_path = os.path.join(path, subdataset, 'train')\n",
    "    anotations_path = os.path.join(train_path, 'annotations')\n",
    "    labels_path = os.path.join(train_path, 'labels')\n",
    "    xmls_path = os.path.join(anotations_path, 'xmls')\n",
    "    file_names = os.listdir(xmls_path)\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        try:\n",
    "            file_name = file_name.split('.')[0]\n",
    "            # If labels folder doesn't exist, create it\n",
    "            if not os.path.exists(labels_path):\n",
    "                os.makedirs(labels_path)\n",
    "            with open(os.path.join(labels_path, file_name + '.txt'), 'w') as file:\n",
    "                with open(os.path.join(xmls_path, file_name + '.xml'), 'r') as xml:\n",
    "                    tree = ET.parse(xml)\n",
    "                    root = tree.getroot()\n",
    "                    # Get the width and height of the image to normalize the anotations\n",
    "                    size = root.find('size')\n",
    "                    img_width = int(size.find('width').text)\n",
    "                    img_height = int(size.find('height').text)\n",
    "\n",
    "                    # Write the anotations in YOLO format\n",
    "                    for obj in root.findall('object'):\n",
    "                        # Class id:\n",
    "                        class_name = obj.find('name').text\n",
    "                        class_id = class_name_to_id_dict[class_name]\n",
    "                        # Bounding box:\n",
    "                        bbox = obj.find('bndbox')\n",
    "                        x_center = (int(bbox.find('xmin').text) + int(bbox.find('xmax').text)) / 2\n",
    "                        y_center = (int(bbox.find('ymin').text) + int(bbox.find('ymax').text)) / 2\n",
    "                        width = int(bbox.find('xmax').text) - int(bbox.find('xmin').text) \n",
    "                        height = int(bbox.find('ymax').text) - int(bbox.find('ymin').text)\n",
    "                        # Normalize the values\n",
    "                        x_center /= img_width\n",
    "                        y_center /= img_height\n",
    "                        width /= img_width\n",
    "                        height /= img_height\n",
    "                        # Write the anotation\n",
    "                        file.write(f'{class_id} {x_center} {y_center} {width} {height}\\n')\n",
    "                       \n",
    "        except Exception as e:\n",
    "            # Most of the errors are due to label not in ['D00', 'D10', 'D20', 'D40']\n",
    "            # print(f'Error with {file_name}: {e}')\n",
    "            problematic_files.append(file_name)\n",
    "            continue\n",
    "\n",
    "    # Remove the anotations folder\n",
    "    if os.path.exists(anotations_path):\n",
    "        os.system(f'rm -r {anotations_path}')\n",
    "\n",
    "    # Create dataset_{subdataset}.yaml in yaml/ folder with YOLO dataset configuration\n",
    "    if not os.path.exists('yaml'):\n",
    "        os.makedirs('yaml')\n",
    "    with open(f'yaml/dataset_{subdataset}.yaml', 'w') as file:\n",
    "        file.write(f'path: ../tfg-informatica/datasets/RDD2022/images\\n')\n",
    "        file.write(f'train: {subdataset}/train/images\\n')\n",
    "        file.write(f'val: {subdataset}/train/images\\n') # We don't have validation data. For now...\n",
    "        # Check if there is a {subdataset}/test folder exist\n",
    "        if os.path.exists(os.path.join(path, subdataset, 'test')):\n",
    "            file.write(f'test: {subdataset}/test/images\\n')\n",
    "        # Class names\n",
    "        file.write(f'\\nnc: 4\\n')\n",
    "        file.write(f'names:\\n  0: D00 - Longitudinal Crack\\n  1: D10 - Transverse Crack\\n  2: D20 - Alligator Crack\\n  3: D40 - Potholes\\n')\n",
    "\n",
    "    # Create dataset_{subdataset}.yaml in yaml_gdrive/ folder with YOLO dataset configuration with the paths in MY Google Drive\n",
    "    if not os.path.exists('yaml_gdrive'):\n",
    "        os.makedirs('yaml_gdrive')\n",
    "    with open(f'yaml_gdrive/dataset_{subdataset}.yaml', 'w') as file:\n",
    "        file.write(f'path: gdrive/MyDrive/tfg-informatica/datasets/RDD2022/images\\n')\n",
    "        file.write(f'train: {subdataset}/train/images\\n')\n",
    "        file.write(f'val: {subdataset}/train/images\\n')\n",
    "        # Check if there is a {subdataset}/test folder exist\n",
    "        if os.path.exists(os.path.join(path, subdataset, 'test')):\n",
    "            file.write(f'test: {subdataset}/test/images\\n')\n",
    "        # Class names\n",
    "        file.write(f'\\nnc: 4\\n')\n",
    "        file.write(f'names:\\n  0: D00 - Longitudinal Crack\\n  1: D10 - Transverse Crack\\n  2: D20 - Alligator Crack\\n  3: D40 - Potholes\\n')\n",
    "\n",
    "print(f'Num. Problematic files: {len(problematic_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a dataset_All.yaml with all the subdatasets\n",
    "# Create dataset_All.yaml in yaml/ folder with YOLO dataset configuration\n",
    "if not os.path.exists('yaml'):\n",
    "        os.makedirs('yaml')\n",
    "with open(f'yaml/dataset_All.yaml', 'w') as file:\n",
    "    file.write(f'path: ../tfg-informatica/datasets/RDD2022/images\\n')\n",
    "    file.write(f'train: \\n')\n",
    "    for subdataset in subdatasets:\n",
    "        train_path = os.path.join(path, subdataset, 'train')\n",
    "        if os.path.exists(train_path):\n",
    "            file.write(f'- {subdataset}/train/images\\n')\n",
    "    file.write(f'val: \\n')\n",
    "    for subdataset in subdatasets:\n",
    "        train_path = os.path.join(path, subdataset, 'train')\n",
    "        if os.path.exists(train_path):\n",
    "            file.write(f'- {subdataset}/train/images\\n')\n",
    "    # Class names\n",
    "    file.write(f'\\nnc: 4\\n')\n",
    "    file.write(f'names:\\n  0: D00 - Longitudinal Crack\\n  1: D10 - Transverse Crack\\n  2: D20 - Alligator Crack\\n  3: D40 - Potholes\\n')\n",
    "\n",
    "# Create dataset_All.yaml in yaml_gdrive/ folder with YOLO dataset configuration with the paths in MY Google Drive\n",
    "if not os.path.exists('yaml_gdrive'):\n",
    "    os.makedirs('yaml_gdrive')\n",
    "with open(f'yaml_gdrive/dataset_All.yaml', 'w') as file:\n",
    "    file.write(f'path: gdrive/MyDrive/tfg-informatica/datasets/RDD2022/images\\n')\n",
    "    file.write(f'train: \\n')\n",
    "    for subdataset in subdatasets:\n",
    "        train_path = os.path.join(path, subdataset, 'train')\n",
    "        if os.path.exists(train_path):\n",
    "            file.write(f'- {subdataset}/train/images\\n')\n",
    "    file.write(f'val: \\n')\n",
    "    for subdataset in subdatasets:\n",
    "        train_path = os.path.join(path, subdataset, 'train')\n",
    "        if os.path.exists(train_path):\n",
    "            file.write(f'- {subdataset}/train/images\\n')\n",
    "    # Class names\n",
    "    file.write(f'\\nnc: 4\\n')\n",
    "    file.write(f'names:\\n  0: D00 - Longitudinal Crack\\n  1: D10 - Transverse Crack\\n  2: D20 - Alligator Crack\\n  3: D40 - Potholes\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
